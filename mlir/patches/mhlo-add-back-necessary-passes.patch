From b1728b65b1511cd5ef3e11650b9e416d3fad068f Mon Sep 17 00:00:00 2001
From: paul0403 <paulhaochen.wang@gmail.com>
Date: Thu, 29 May 2025 11:00:56 -0400
Subject: [PATCH] restore the removed mhlo passes we need:
 mhlo-legalize-control-flow, mhlo-legalize-to-std, hlo-legalize-sort

---
 mhlo/transforms/CMakeLists.txt                |   6 +
 .../legalize_control_flow.cc                  | 288 +++++++++
 .../transforms/legalize_sort/legalize_sort.cc | 577 ++++++++++++++++++
 .../legalize_to_standard.cc                   | 243 ++++++++
 .../legalize_to_standard_patterns.td          |  92 +++
 mhlo/transforms/mhlo_passes.td                |  19 +
 mhlo/transforms/passes.h                      |   4 +
 7 files changed, 1229 insertions(+)
 create mode 100644 mhlo/transforms/legalize_control_flow/legalize_control_flow.cc
 create mode 100644 mhlo/transforms/legalize_sort/legalize_sort.cc
 create mode 100644 mhlo/transforms/legalize_to_standard/legalize_to_standard.cc
 create mode 100644 mhlo/transforms/legalize_to_standard/legalize_to_standard_patterns.td

diff --git a/mhlo/transforms/CMakeLists.txt b/mhlo/transforms/CMakeLists.txt
index d6848633..26d3b419 100644
--- a/mhlo/transforms/CMakeLists.txt
+++ b/mhlo/transforms/CMakeLists.txt
@@ -26,14 +26,20 @@ set(LLVM_TARGET_DEFINITIONS chlo_legalize_to_hlo/chlo_legalize_to_hlo_patterns.t
 mlir_tablegen(chlo_legalize_to_hlo/generated_chlo_legalize_to_hlo.inc -gen-rewriters)
 add_public_tablegen_target(MLIRChloLegalizeToHloIncGen)
 
+set(LLVM_TARGET_DEFINITIONS legalize_to_standard/legalize_to_standard_patterns.td)
+mlir_tablegen(legalize_to_standard/generated_legalize_to_standard.inc -gen-rewriters)
+add_public_tablegen_target(MLIRMhloLegalizeToStandardIncGen)
 
 
 add_mlir_library(MhloPasses
   collapse_elementwise_map/collapse_elementwise_map.cc
   convert_to_signless/convert_to_signless_pass.cc
   expand_hlo_tuples/expand_hlo_tuples.cc
+  legalize_control_flow/legalize_control_flow.cc
   legalize_dot_to_dot_general/legalize_dot_to_dot_general.cc
   legalize_einsum_to_dot_general/legalize_einsum_to_dot_general.cc
+  legalize_sort/legalize_sort.cc
+  legalize_to_standard/legalize_to_standard.cc
   legalize_torch_index_select_to_gather/legalize_torch_index_select_to_gather.cc
   legalize_trigonometric_to_approximation/legalize_trigonometric_to_approximation.cc
   materialize_broadcasts/materialize_broadcasts.cc
diff --git a/mhlo/transforms/legalize_control_flow/legalize_control_flow.cc b/mhlo/transforms/legalize_control_flow/legalize_control_flow.cc
new file mode 100644
index 00000000..9d473b9a
--- /dev/null
+++ b/mhlo/transforms/legalize_control_flow/legalize_control_flow.cc
@@ -0,0 +1,288 @@
+/* Copyright 2019 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// This file implements logic for lowering MHLO dialect to SCF dialect.
+#include <memory>
+#include <optional>
+#include <utility>
+
+#include "llvm/Support/Casting.h"
+#include "mhlo/IR/hlo_ops.h"
+#include "mhlo/transforms/passes.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"  // TF:llvm-project
+#include "mlir/IR/Block.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/TypeUtilities.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+namespace mhlo {
+
+#define GEN_PASS_DEF_LEGALIZECONTROLFLOWPASS
+#include "mhlo/transforms/mhlo_passes.h.inc"
+
+namespace {
+
+// All transformations in this file take mhlo blocks which end with
+// mhlo::ReturnOp and lower to SCF ops which end with scf::YieldOp. Inline an
+// entire block with the only change being return -> yield.
+void inlineMhloRegionIntoSCFRegion(PatternRewriter& rewriter, Region& mhlo,
+                                   Region& scf) {
+  // Remove an existing block, then move the region over.
+  if (!scf.empty()) rewriter.eraseBlock(&scf.back());
+  rewriter.inlineRegionBefore(mhlo, scf, scf.end());
+  // Fix up the terminator.
+  PatternRewriter::InsertionGuard guard(rewriter);
+  rewriter.setInsertionPointToEnd(&scf.back());
+  auto* terminator = scf.back().getTerminator();
+  rewriter.replaceOpWithNewOp<scf::YieldOp>(terminator,
+                                            terminator->getOperands());
+}
+
+// mhlo ops need inputs to be tensors, but scalar values can be a scalar tensor
+// or a 1 element tensor. To handle this, collapse shape before extracting the
+// scalar value when necessary.
+Value extractTensorValue(OpBuilder& b, Value tensor) {
+  auto loc = tensor.getLoc();
+  if (mlir::cast<TensorType>(tensor.getType()).hasRank() &&
+      mlir::cast<TensorType>(tensor.getType()).getRank() != 0) {
+    tensor = b.create<tensor::CollapseShapeOp>(
+        loc, tensor, SmallVector<ReassociationIndices>());
+  }
+  return b.create<tensor::ExtractOp>(loc, tensor, ValueRange());
+}
+
+struct ScfForBounds {
+  Value lb;
+  Value ub;
+  Value step;
+  unsigned indexArgIndex;
+};
+
+std::optional<ScfForBounds> extractForBounds(mhlo::WhileOp op) {
+  auto& cond = op.getCond().front();
+  auto& body = op.getBody().front();
+  if (cond.getOperations().size() != 2) return std::nullopt;
+
+  auto matchBbArg = [](Value v, Block& block) -> std::optional<unsigned> {
+    if (!mlir::isa<BlockArgument>(v) || v.getParentBlock() != &block)
+      return std::nullopt;
+    return mlir::cast<BlockArgument>(v).getArgNumber();
+  };
+
+  auto compare = llvm::dyn_cast<mhlo::CompareOp>(cond.front());
+  // If the rhs of the comapare is defined outside the block, it's a constant
+  // within the loop.
+  if (!compare ||
+      compare.getComparisonDirection() != mhlo::ComparisonDirection::LT ||
+      compare.getRhs().getParentBlock() == &cond ||
+      !getElementTypeOrSelf(compare.getLhs().getType())
+           .isSignlessIntOrIndex()) {
+    return std::nullopt;
+  }
+
+  auto iterArg = matchBbArg(compare.getLhs(), cond);
+  if (!iterArg) return std::nullopt;
+
+  auto add = llvm::dyn_cast_or_null<mhlo::AddOp>(
+      body.getTerminator()->getOperand(*iterArg).getDefiningOp());
+  if (!add || matchBbArg(add.getLhs(), body) != iterArg ||
+      add.getRhs().getParentBlock() == &body) {
+    return std::nullopt;
+  }
+
+  ScfForBounds bounds;
+  bounds.ub = compare.getRhs();
+  bounds.step = add.getRhs();
+  bounds.lb = op->getOperand(*iterArg);
+  bounds.indexArgIndex = *iterArg;
+  return bounds;
+}
+
+// Rewrites `mhlo.while` to `scf.while` or `scf.for`.
+struct WhileOpPattern : public OpConversionPattern<mhlo::WhileOp> {
+  using OpConversionPattern<WhileOp>::OpConversionPattern;
+
+  LogicalResult matchAndRewrite(
+      mhlo::WhileOp op, OpAdaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    auto loc = op.getLoc();
+
+    if (auto bounds = extractForBounds(op)) {
+      auto newForOp = rewriter.create<scf::ForOp>(
+          loc, extractTensorValue(rewriter, bounds->lb),
+          extractTensorValue(rewriter, bounds->ub),
+          extractTensorValue(rewriter, bounds->step), adaptor.getOperands());
+
+      rewriter.setInsertionPointToEnd(newForOp.getBody());
+      // Inline while body, and only replace the mhlo.return with an scf.yield.
+      inlineMhloRegionIntoSCFRegion(rewriter, op.getBody(),
+                                    newForOp.getRegion());
+      auto indexArg = newForOp.getRegion().insertArgument(
+          unsigned{0}, newForOp.getLowerBound().getType(), loc);
+      auto oldIndexArg =
+          newForOp.getRegion().getArgument(1 + bounds->indexArgIndex);
+      rewriter.setInsertionPointToStart(&newForOp.getRegion().front());
+      auto indexArgTensor = rewriter.create<tensor::FromElementsOp>(
+          loc, oldIndexArg.getType(), indexArg);
+      oldIndexArg.replaceAllUsesWith(indexArgTensor);
+
+      rewriter.replaceOp(op, newForOp.getResults());
+      return success();
+    }
+
+    auto newWhileOp = rewriter.create<scf::WhileOp>(loc, op.getResultTypes(),
+                                                    adaptor.getOperands());
+
+    // Inline while condition. The block is the same, except the boolean result
+    // needs to be extracted and used with an scf.condition.
+    rewriter.inlineRegionBefore(op.getCond(), newWhileOp.getBefore(),
+                                newWhileOp.getBefore().end());
+    auto conditionReturn =
+        cast<mhlo::ReturnOp>(newWhileOp.getBefore().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newWhileOp.getBefore().front());
+    Value i1 = extractTensorValue(rewriter, conditionReturn->getOperand(0));
+    rewriter.replaceOpWithNewOp<scf::ConditionOp>(
+        conditionReturn, i1, newWhileOp.getBeforeArguments());
+
+    // Inline while body, and only replace the mhlo.return with an scf.yield.
+    inlineMhloRegionIntoSCFRegion(rewriter, op.getBody(),
+                                  newWhileOp.getAfter());
+
+    rewriter.replaceOp(op, newWhileOp.getResults());
+    return success();
+  }
+};
+
+// Rewrites `mhlo.if` to `scf.if`.
+struct IfOpPattern : public OpConversionPattern<mhlo::IfOp> {
+  using OpConversionPattern<IfOp>::OpConversionPattern;
+
+  LogicalResult matchAndRewrite(
+      mhlo::IfOp op, OpAdaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    auto scfIf = rewriter.create<scf::IfOp>(
+        op.getLoc(), op.getResultTypes(),
+        extractTensorValue(rewriter, adaptor.getPred()),
+        /*withElseRegion=*/true);
+    inlineMhloRegionIntoSCFRegion(rewriter, op.getTrueBranch(),
+                                  scfIf.getThenRegion());
+    inlineMhloRegionIntoSCFRegion(rewriter, op.getFalseBranch(),
+                                  scfIf.getElseRegion());
+    rewriter.replaceOp(op, scfIf.getResults());
+    return success();
+  }
+};
+
+// Rewrites `mhlo.case` to a nested `scf.if`.
+struct CaseOpPattern : public OpConversionPattern<mhlo::CaseOp> {
+  using OpConversionPattern<CaseOp>::OpConversionPattern;
+
+  // Recursively create if/else ops to handle each possible value in a case op.
+  scf::IfOp createNestedCases(int currentIdx, CaseOp op, OpAdaptor adaptor,
+                              PatternRewriter& outerBuilder) const {
+    Location loc = op.getLoc();
+    Value idxValue = adaptor.getIndex();
+    auto finalIdx = op.getBranches().size() - 2;
+
+    // Determine if the current index matches the case index.
+    auto scalarType = idxValue.getType();
+    auto shapedType = mlir::cast<ShapedType>(scalarType);
+    auto constAttr = DenseElementsAttr::get(
+        shapedType, {mlir::cast<mlir::Attribute>(
+                        outerBuilder.getI32IntegerAttr(currentIdx))});
+    Value currentIdxVal = outerBuilder.create<mhlo::ConstantOp>(
+        loc, idxValue.getType(), constAttr);
+
+    auto scfIf = outerBuilder.create<scf::IfOp>(
+        loc, op.getResultTypes(),
+        extractTensorValue(outerBuilder, outerBuilder.create<mhlo::CompareOp>(
+                                             loc, idxValue, currentIdxVal,
+                                             ComparisonDirection::EQ)),
+        /*withElseRegion=*/true);
+    inlineMhloRegionIntoSCFRegion(outerBuilder, op.getBranches()[currentIdx],
+                                  scfIf.getThenRegion());
+    int nextIdx = currentIdx + 1;
+    // Don't recurse for the final default block.
+    if (currentIdx == static_cast<int64_t>(finalIdx)) {
+      inlineMhloRegionIntoSCFRegion(outerBuilder, op.getBranches()[nextIdx],
+                                    scfIf.getElseRegion());
+    } else {
+      PatternRewriter::InsertionGuard guard(outerBuilder);
+      outerBuilder.setInsertionPointToEnd(&scfIf.getElseRegion().back());
+      auto innerIf = createNestedCases(nextIdx, op, adaptor, outerBuilder);
+      outerBuilder.create<scf::YieldOp>(op.getLoc(), innerIf.getResults());
+    }
+    return scfIf;
+  }
+
+  LogicalResult matchAndRewrite(
+      mhlo::CaseOp op, OpAdaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    // Inline the op if there is only a default block.
+    if (op.getBranches().size() == 1) {
+      Block& block = op.getBranches().front().front();
+      auto results = block.getTerminator()->getOperands();
+      // Remove the mhlo.return terminator, then inline the block.
+      rewriter.eraseOp(block.getTerminator());
+      rewriter.inlineBlockBefore(/*source=*/&block, /*dest=*/op.getOperation(),
+                                 /*argValues=*/{});
+      rewriter.replaceOp(op, results);
+      return success();
+    }
+
+    // Begin recursion with case 0.
+    rewriter.replaceOp(
+        op, createNestedCases(0, op, adaptor, rewriter).getResults());
+    return success();
+  }
+};
+
+struct LegalizeControlFlowPass
+    : public impl::LegalizeControlFlowPassBase<LegalizeControlFlowPass> {
+  // Perform the lowering to MLIR control flow.
+  void runOnOperation() override {
+    func::FuncOp f = getOperation();
+    MLIRContext* ctx = f.getContext();
+
+    RewritePatternSet patterns(&getContext());
+    patterns.add<WhileOpPattern, IfOpPattern, CaseOpPattern>(&getContext());
+
+    mlir::ConversionTarget target(*ctx);
+    target.markUnknownOpDynamicallyLegal([](Operation*) { return true; });
+    target.addIllegalOp<mhlo::IfOp, mhlo::WhileOp, mhlo::CaseOp>();
+
+    if (failed(applyPartialConversion(f, target, std::move(patterns)))) {
+      signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mhlo
+}  // namespace mlir
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
+mlir::mhlo::createLegalizeControlFlowPass() {
+  return std::make_unique<LegalizeControlFlowPass>();
+}
diff --git a/mhlo/transforms/legalize_sort/legalize_sort.cc b/mhlo/transforms/legalize_sort/legalize_sort.cc
new file mode 100644
index 00000000..8ba9de9a
--- /dev/null
+++ b/mhlo/transforms/legalize_sort/legalize_sort.cc
@@ -0,0 +1,577 @@
+/* Copyright 2019 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// This file implements logic for lowering mhlo.sort to the SCF dialect.
+#include <iterator>
+#include <memory>
+#include <utility>
+
+#include "llvm/ADT/STLExtras.h"
+#include "mhlo/IR/hlo_ops.h"
+#include "mhlo/transforms/passes.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"  // TF:llvm-project
+#include "mlir/IR/Block.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/IRMapping.h"
+#include "mlir/IR/ImplicitLocOpBuilder.h"
+#include "mlir/IR/Location.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/TypeRange.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+namespace mhlo {
+
+#define GEN_PASS_DEF_HLOLEGALIZESORTPASS
+#include "mhlo/transforms/mhlo_passes.h.inc"
+
+namespace {
+
+using ::mlir::arith::AddIOp;
+using ::mlir::arith::MinSIOp;
+using ::mlir::arith::SelectOp;
+
+constexpr int64_t kInsertionSortSize = 16;
+
+// Inlines the `comparator` region (without terminator) at the current insertion
+// point, replacing the arguments with the given values from `lhs` and `rhs`.
+Value emitComparison(ImplicitLocOpBuilder& b, SmallVector<Value>& lhs,
+                     SmallVector<Value>& rhs, Region& comparator) {
+  assert(comparator.hasOneBlock() && "Comparator must have only one block.");
+  Block& block = comparator.front();
+  assert(block.getTerminator()->getOperands().size() == 1 &&
+         "Comparator must return a single value");
+
+  IRMapping mapping;
+  for (auto [idx, arg] : llvm::enumerate(comparator.getArguments())) {
+    Value value = idx % 2 == 0 ? lhs[idx / 2] : rhs[idx / 2];
+    Type type = RankedTensorType::get({}, value.getType());
+    mapping.map(arg, b.create<tensor::FromElementsOp>(type, value));
+  }
+
+  for (Operation& op : block.without_terminator()) b.clone(op, mapping);
+  Value result = mapping.lookup(block.getTerminator()->getOperands().front());
+
+  return b.create<tensor::ExtractOp>(result, ValueRange());
+}
+
+// Emits a binary search of `pivots` in `arrayMemrefs` (all rank 1) in the range
+// [`left`;`right`). `arrayMemrefs` must be sorted according to `comparator`.
+Value emitBinarySearch(ImplicitLocOpBuilder& b, Value leftInit, Value rightInit,
+                       SmallVector<Value>& pivots, ValueRange arrayMemrefs,
+                       Region& comparator) {
+  SmallVector<Type, 2> types{leftInit.getType(), rightInit.getType()};
+  ArithBuilder arith(b, b.getLoc());
+
+  // while (
+  auto whileOp =
+      b.create<scf::WhileOp>(types, SmallVector<Value, 2>{leftInit, rightInit});
+  OpBuilder::InsertionGuard guard(b);
+
+  //        left < right) {
+  Block* before = b.createBlock(&whileOp.getBefore(), {}, types,
+                                {whileOp.getLoc(), whileOp.getLoc()});
+  {
+    Value left = before->getArgument(0), right = before->getArgument(1);
+    b.setInsertionPointToEnd(before);
+    b.create<scf::ConditionOp>(arith.slt(left, right), before->getArguments());
+  }
+
+  Block* after = b.createBlock(&whileOp.getAfter(), {}, types,
+                               {whileOp.getLoc(), whileOp.getLoc()});
+  {
+    Value left = after->getArgument(0), right = after->getArgument(1);
+    b.setInsertionPointToEnd(after);
+    //   int mid = (left + right) >> 1;
+    Value one = b.create<arith::ConstantIndexOp>(1);
+    Value mid = b.create<arith::ShRUIOp>(arith.add(left, right), one);
+    Value midPlusOne = b.create<AddIOp>(mid, one);
+
+    auto arraysAtMid = llvm::to_vector(
+        llvm::map_range(arrayMemrefs, [&](Value arrayMemref) -> Value {
+          return b.create<memref::LoadOp>(arrayMemref, mid);
+        }));
+    Value cond = emitComparison(b, pivots, arraysAtMid, comparator);
+    //   if (comparator(pivot, array[mid]))
+    //     right = mid;
+    //   else
+    //     left = mid + 1;
+    Value newLeft = arith.select(cond, left, midPlusOne);
+    Value newRight = arith.select(cond, mid, right);
+
+    // }
+    b.create<scf::YieldOp>(ValueRange{newLeft, newRight});
+  }
+
+  return whileOp.getResult(0);
+}
+
+SmallVector<Value> loadTensorElements(ImplicitLocOpBuilder& b,
+                                      ValueRange tensors, Value index) {
+  return llvm::to_vector(llvm::map_range(tensors, [&](Value tensor) -> Value {
+    return b.create<tensor::ExtractOp>(tensor, index);
+  }));
+}
+
+SmallVector<Value> loadMemrefElements(ImplicitLocOpBuilder& b,
+                                      ValueRange memrefs, Value index) {
+  return llvm::to_vector(llvm::map_range(memrefs, [&](Value memref) -> Value {
+    Type type = mlir::cast<MemRefType>(memref.getType()).getElementType();
+    return b.create<memref::LoadOp>(type, memref, index);
+  }));
+}
+
+void storeMemrefElements(ImplicitLocOpBuilder& b, ValueRange memrefs,
+                         Value index, ValueRange values) {
+  for (auto [value, memref] : llvm::zip(values, memrefs)) {
+    b.create<memref::StoreOp>(value, memref, index);
+  }
+}
+
+// Insertion sorts `inputTensors` in the range [`lo`; `hi`), storing the results
+// in `outputMemrefs`. `inputTensors` and `outputMemrefs` must all be rank 1 and
+// of identical size.
+void emitInsertionSort(ImplicitLocOpBuilder& b, Value lo, Value hi,
+                       ValueRange inputTensors, ValueRange outputMemrefs,
+                       mlir::Region& comparator) {
+  ArithBuilder arith(b, b.getLoc());
+  Value zero = b.create<arith::ConstantIndexOp>(0);
+  Value one = b.create<arith::ConstantIndexOp>(1);
+
+  // array[lo] = tensors[lo];
+  storeMemrefElements(b, outputMemrefs, lo,
+                      loadTensorElements(b, inputTensors, lo));
+
+  // for (int start = lo + 1; start < hi; ++start)
+  {
+    auto forOp = b.create<scf::ForOp>(arith.add(lo, one), hi, one);
+    OpBuilder::InsertionGuard outerGuard(b);
+    b.setInsertionPointToStart(forOp.getBody());
+    Value start = forOp.getInductionVar();
+
+    //   T pivot = tensors[start];
+    auto pivots = loadTensorElements(b, inputTensors, start);
+
+    //   int index = binarySearch(lo, start, pivot, array, comparator);
+    auto index =
+        emitBinarySearch(b, lo, start, pivots, outputMemrefs, comparator);
+
+    //   int n = start - index;  // The number of elements to move
+    Value n = arith.sub(start, index);
+
+    // memmove(&array[index + 1], &array[index], n * sizeof(T))
+    // memref::CopyOp would be nice to use here, but:
+    // 1. It lowers to a quite inefficient library call in the general case
+    //    (strides != 1).
+    // 2. It implements memcpy semantics, but we need memmove here.
+    // So we go with a loop instead.
+    auto copyForOp = b.create<scf::ForOp>(zero, n, one);
+    {
+      OpBuilder::InsertionGuard innerGuard(b);
+      b.setInsertionPointToStart(copyForOp.getBody());
+      Value copyLoopIndex = copyForOp.getBody()->getArgument(0);
+
+      Value dstIndex = arith.sub(start, copyLoopIndex);
+      Value srcIndex = arith.sub(dstIndex, one);
+      storeMemrefElements(b, outputMemrefs, dstIndex,
+                          loadMemrefElements(b, outputMemrefs, srcIndex));
+    }
+    //   array[index] = pivot;
+    storeMemrefElements(b, outputMemrefs, index, pivots);
+  }
+}
+
+void emitMerge(ImplicitLocOpBuilder& b, Value lo, Value mid, Value hi,
+               ValueRange readBufs, ValueRange writeBufs,
+               mlir::Region& comparator) {
+  ArithBuilder arith(b, b.getLoc());
+  // The while loop runs until we reach the end of either interval. It has three
+  // loop-carried variables:
+  // 1. current output index
+  // 2. current read index for interval 1
+  // 3. current read index for interval 2
+  SmallVector<Type> whileArgTypes{lo.getType(), lo.getType(), mid.getType()};
+  SmallVector<Value> whileInitArgs{lo, lo, mid};
+  SmallVector<Location> whileArgLocs(whileArgTypes.size(), b.getLoc());
+
+  // while(
+  auto whileOp = b.create<scf::WhileOp>(whileArgTypes, whileInitArgs);
+  {
+    OpBuilder::InsertionGuard guard(b);
+    {
+      Block* before =
+          b.createBlock(&whileOp.getBefore(), {}, whileArgTypes, whileArgLocs);
+      Value i0 = before->getArgument(1), i1 = before->getArgument(2);
+      b.setInsertionPointToEnd(before);
+
+      //     i0 < mid && i1 < hi) {
+      Value inbounds0 = arith.slt(i0, mid);
+      Value inbounds1 = arith.slt(i1, hi);
+
+      b.create<scf::ConditionOp>(arith._and(inbounds0, inbounds1),
+                                 before->getArguments());
+    }
+
+    {
+      Block* after =
+          b.createBlock(&whileOp.getAfter(), {}, whileArgTypes, whileArgLocs);
+      Value iOut = after->getArgument(0), i0 = after->getArgument(1),
+            i1 = after->getArgument(2);
+      b.setInsertionPointToEnd(after);
+
+      //   auto vals0 = readBufs[i0], vals1 = readBufs[i1];
+      SmallVector<Value> vals0 = loadMemrefElements(b, readBufs, i0);
+      SmallVector<Value> vals1 = loadMemrefElements(b, readBufs, i1);
+
+      //   writeBufs[iOut] = comparator(vals1, vals0)
+      //                       ? readBufs[i1++] : readBufs[i0++];
+      Value cmp = emitComparison(b, vals1, vals0, comparator);
+      SmallVector<Value> pickedVals;
+      for (auto [val0, val1] : llvm::zip(vals0, vals1)) {
+        pickedVals.push_back(b.create<SelectOp>(cmp, val1, val0));
+      }
+      storeMemrefElements(b, writeBufs, iOut, pickedVals);
+
+      Value one = b.create<arith::ConstantIndexOp>(1);
+      Value nexti0 = b.create<SelectOp>(cmp, i0, arith.add(i0, one));
+      Value nexti1 = b.create<SelectOp>(cmp, arith.add(i1, one), i1);
+      //   ++iOut;
+      Value nextIOut = b.create<AddIOp>(iOut, one);
+      b.create<scf::YieldOp>(ValueRange{nextIOut, nexti0, nexti1});
+    }
+  }
+
+  // At this point, exactly one of the input ranges will have leftover elements.
+  Value iOut = whileOp->getResult(0);
+  Value i0 = whileOp->getResult(1);
+  Value i1 = whileOp->getResult(2);
+
+  // We could use memref::CopyOp here, but typically, there aren't many leftover
+  // elements for randomly shuffled inputs.
+  Value leftoverIn0 = arith.slt(i0, mid);
+  Value start = arith.select(leftoverIn0, i0, i1);
+  Value end = arith.select(leftoverIn0, mid, hi);
+  Value n = arith.sub(end, start);
+
+  Value zero = b.create<arith::ConstantIndexOp>(0);
+  Value one = b.create<arith::ConstantIndexOp>(1);
+  auto forOp = b.create<scf::ForOp>(zero, n, one);
+  b.setInsertionPointToStart(forOp.getBody());
+  Value copyIndex = forOp.getBody()->getArgument(0);
+
+  Value srcIndex = arith.add(start, copyIndex);
+  Value dstIndex = arith.add(iOut, copyIndex);
+  storeMemrefElements(b, writeBufs, dstIndex,
+                      loadMemrefElements(b, readBufs, srcIndex));
+}
+
+// Emits a bottom up merge sort of `inputTensors` in the range [`lo`; `hi`), and
+// writes the results to either `outputs0` or `outputs1`.
+// Returns 0 if the results are in `outputs0`, 1 if they are in `outputs1`.
+// TODO(jreiffers): Consider implementing top-down merge sort.
+Value emitBottomUpMergeSort(ImplicitLocOpBuilder& b, Value lo, Value hi,
+                            int64_t staticSortDimSize, ValueRange inputTensors,
+                            ValueRange outputs0, ValueRange outputs1,
+                            mlir::Region& comparator) {
+  ArithBuilder arith(b, b.getLoc());
+  Value size = arith.sub(hi, lo);
+
+  Value zero = b.create<arith::ConstantIndexOp>(0);
+  Value insertionSortSize =
+      b.create<arith::ConstantIndexOp>(kInsertionSortSize);
+
+  // Run insertion sort on blocks of size kInsertionSortSize.
+  // for (int start = 0; start < size; start += kInsertionSortSize) {
+  {
+    auto forOp = b.create<scf::ForOp>(zero, size, insertionSortSize);
+    OpBuilder::InsertionGuard guard(b);
+    b.setInsertionPointToStart(forOp.getBody());
+    Value start = forOp.getBody()->getArgument(0);
+    Value end = arith.add(
+        b.create<MinSIOp>(arith.add(start, insertionSortSize), size), lo);
+    emitInsertionSort(b, start, end, inputTensors, outputs0, comparator);
+  }
+
+  Value initParity = b.create<arith::ConstantIntOp>(0, 1);
+  if (staticSortDimSize >= 0 && staticSortDimSize < kInsertionSortSize) {
+    return initParity;
+  }
+
+  // The while arguments are:
+  // 1. the current size
+  // 2. the original index of the buffers we're currently reading from
+  // 3. the buffers we're currently reading from
+  // 4. the buffers we're currently writing to.
+  //
+  // 1 gets doubled each iteration, 2 gets negated, 3 and 4 are swapped.
+  // int currentSize = 16;
+  SmallVector<Value> whileInitArgs{insertionSortSize, initParity};
+  // First we read from `outputs0` (initialized by the insertion sort above).
+  llvm::copy(outputs0, std::back_inserter(whileInitArgs));
+  llvm::copy(outputs1, std::back_inserter(whileInitArgs));
+
+  SmallVector<Type> whileArgTypes;
+  for (auto val : whileInitArgs) whileArgTypes.push_back(val.getType());
+
+  SmallVector<Location> whileArgLocs(whileArgTypes.size(), b.getLoc());
+
+  // while (
+  auto whileOp = b.create<scf::WhileOp>(whileArgTypes, whileInitArgs);
+  OpBuilder::InsertionGuard guard(b);
+
+  //        currentSize < totalSize)
+  {
+    Block* before =
+        b.createBlock(&whileOp.getBefore(), {}, whileArgTypes, whileArgLocs);
+    Value currentSize = before->getArgument(0);
+    b.setInsertionPointToEnd(before);
+    b.create<scf::ConditionOp>(arith.slt(currentSize, size),
+                               before->getArguments());
+  }
+
+  size_t numArgs = inputTensors.size();
+  //                                 {
+  {
+    Block* after =
+        b.createBlock(&whileOp.getAfter(), {}, whileArgTypes, whileArgLocs);
+
+    Value currentSize = after->getArgument(0);
+    Value parity = after->getArgument(1);
+    auto readBufs = after->getArguments().drop_front(2).take_front(numArgs);
+    auto writeBufs = after->getArguments().take_back(numArgs);
+
+    Value twoCurrentSize = arith.add(currentSize, currentSize);
+
+    // for (int start = 0; start < size; start += 2*currentSize) {
+    {
+      auto forOp = b.create<scf::ForOp>(zero, size, twoCurrentSize);
+      b.setInsertionPointToStart(forOp.getBody());
+      Value start = forOp.getBody()->getArgument(0);
+
+      Value mid = b.create<MinSIOp>(size, arith.add(start, currentSize));
+      Value end = b.create<MinSIOp>(size, arith.add(start, twoCurrentSize));
+      emitMerge(b, start, mid, end, readBufs, writeBufs, comparator);
+      b.setInsertionPointAfter(forOp);
+    }
+    // }
+
+    // parity = !parity;
+    Value one = b.create<arith::ConstantIntOp>(1, 1);
+    Value notParity = arith.sub(one, parity);
+    // currentSize *= 2;
+    SmallVector<Value> nextWhileArgs{twoCurrentSize, notParity};
+    llvm::copy(writeBufs, std::back_inserter(nextWhileArgs));
+    llvm::copy(readBufs, std::back_inserter(nextWhileArgs));
+    b.create<scf::YieldOp>(nextWhileArgs);
+  }
+  // }
+
+  // The result is the parity bit.
+  return whileOp.getResults().drop_front(1).front();
+}
+
+// Helper struct for extracting 1d slices from tensors and memrefs.
+struct Slicer {
+  Slicer(OpBuilder& b, uint64_t sortDim, Value sortDimSize, ValueRange ivs)
+      : sizes(ivs.size() + 1, b.getI64IntegerAttr(1)),
+        strides(ivs.size() + 1, b.getI64IntegerAttr(1)) {
+    sizes[sortDim] = sortDimSize;
+    for (size_t i = 0; i < ivs.size() + 1; ++i) {
+      if (i == sortDim) {
+        offsets.push_back(b.getI64IntegerAttr(0));
+      } else {
+        offsets.push_back(ivs[i - static_cast<int>(i > sortDim)]);
+      }
+    }
+  }
+
+  RankedTensorType toSlicedType(RankedTensorType sourceType) {
+    return tensor::ExtractSliceOp::inferCanonicalRankReducedResultType(
+        /*resultRank=*/1, sourceType, offsets, sizes, strides);
+  }
+
+  MemRefType toSlicedType(MemRefType sourceType) {
+    return mlir::cast<MemRefType>(memref::SubViewOp::inferRankReducedResultType(
+        {ShapedType::kDynamic} /*1D output*/, sourceType, offsets, sizes,
+        strides));
+  }
+
+  template <typename Op, typename Ty>
+  Value slice(ImplicitLocOpBuilder& b, Value input) {
+    Ty ty = mlir::cast<Ty>(input.getType());
+    return b.create<Op>(toSlicedType(ty), input, offsets, sizes, strides)
+        .getResult();
+  }
+
+  Value apply(ImplicitLocOpBuilder& b, Value input) {
+    Type inTy = input.getType();
+    if (mlir::isa<RankedTensorType>(inTy)) {
+      return slice<tensor::ExtractSliceOp, RankedTensorType>(b, input);
+    }
+    assert(mlir::isa<MemRefType>(inTy));
+    return slice<memref::SubViewOp, MemRefType>(b, input);
+  }
+
+  SmallVector<OpFoldResult> offsets;
+  SmallVector<OpFoldResult> sizes;
+  SmallVector<OpFoldResult> strides;
+};
+
+SmallVector<Value> sliceMemrefsOrTensors(ImplicitLocOpBuilder& b,
+                                         SmallVector<Value>& ivs,
+                                         Value sortDimSize,
+                                         ValueRange memrefsOrTensors,
+                                         SortOp op) {
+  if (ivs.empty()) return memrefsOrTensors;
+
+  SmallVector<Value> outputs;
+  Slicer slicer(b, op.getDimension(), sortDimSize, ivs);
+  // Create subviews/slices.
+  for (Value out : memrefsOrTensors) {
+    outputs.push_back(slicer.apply(b, out));
+  }
+
+  return outputs;
+}
+
+struct SortOpPattern : public OpRewritePattern<SortOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(SortOp op,
+                                PatternRewriter& rewriter) const override {
+    ImplicitLocOpBuilder b(op.getLoc(), rewriter);
+
+    // Note: the output memrefs aren't necessarily the ones that we return,
+    SmallVector<Value> outputMemrefs;
+    SmallVector<Value> scratchMemrefs;
+
+    Value firstOperand = op.getOperands().front();
+    auto firstOperandType = mlir::cast<ShapedType>(firstOperand.getType());
+    int64_t inputRank = firstOperandType.getRank();
+
+    Value sortDimSize = b.createOrFold<tensor::DimOp>(
+        firstOperand, b.create<arith::ConstantIndexOp>(op.getDimension()));
+    int64_t staticSortDimSize = firstOperandType.getDimSize(op.getDimension());
+
+    SmallVector<Value> dynamicDims;
+    for (int i = 0; i < inputRank; ++i) {
+      if (!firstOperandType.isDynamicDim(i)) continue;
+      Value index = b.create<arith::ConstantIndexOp>(i);
+      Value dimOp = b.create<tensor::DimOp>(firstOperand, index);
+      dynamicDims.push_back(dimOp);
+    }
+
+    // Allocate output and scratch memrefs. If the size of the sort dimension is
+    // statically known to be <= kInsertionSortSize, `scratchMemrefs` are unused
+    // and will be cleaned up later.
+    for (auto input : op.getOperands()) {
+      auto inputType = mlir::cast<ShapedType>(input.getType());
+      auto memRefType =
+          MemRefType::get(inputType.getShape(), inputType.getElementType());
+
+      outputMemrefs.push_back(
+          b.create<memref::AllocOp>(memRefType, dynamicDims));
+      scratchMemrefs.push_back(
+          b.create<memref::AllocOp>(memRefType, dynamicDims));
+    }
+
+    b.setInsertionPoint(op);
+    Value zero = b.create<arith::ConstantIndexOp>(0);
+    Value one = b.create<arith::ConstantIndexOp>(1);
+
+    Value forInitArg = b.create<arith::ConstantIntOp>(0, 1);
+    SmallVector<scf::ForOp> forOps;
+    SmallVector<Value> ivs;
+    forOps.reserve(inputRank - 1);
+    ivs.reserve(inputRank - 1);
+    for (int64_t i = 0; i < inputRank; ++i) {
+      if (i != static_cast<int64_t>(op.getDimension())) {
+        Value dim = b.create<arith::ConstantIndexOp>(i);
+        Value ub = b.create<tensor::DimOp>(firstOperand, dim);
+        scf::ForOp& forOp = forOps.emplace_back(
+            b.create<scf::ForOp>(zero, ub, one, ValueRange{forInitArg}));
+        ivs.push_back(forOp.getInductionVar());
+        b.setInsertionPointToStart(&forOp.getRegion().front());
+      }
+    }
+    SmallVector<Value> inputs =
+        sliceMemrefsOrTensors(b, ivs, sortDimSize, op.getOperands(), op);
+    SmallVector<Value> outputs =
+        sliceMemrefsOrTensors(b, ivs, sortDimSize, outputMemrefs, op);
+    SmallVector<Value> scratches =
+        sliceMemrefsOrTensors(b, ivs, sortDimSize, scratchMemrefs, op);
+
+    Value parity =
+        emitBottomUpMergeSort(b, zero, sortDimSize, staticSortDimSize, inputs,
+                              outputs, scratches, op.getRegion());
+
+    // Pass the parity bit through the for loops.
+    for (auto i = static_cast<int64_t>(forOps.size() - 1); i >= 0; --i) {
+      b.setInsertionPointToEnd(&forOps[i].getRegion().front());
+      b.create<scf::YieldOp>(ValueRange{parity});
+      parity = forOps[i]->getResult(0);
+    }
+    b.setInsertionPoint(op);
+
+    SmallVector<Value> outputTensors;
+    for (auto [out0, out1] : llvm::zip(outputMemrefs, scratchMemrefs)) {
+      outputTensors.push_back(b.create<bufferization::ToTensorOp>(
+          b.create<SelectOp>(parity, out1, out0), /*restrict=*/true));
+    }
+
+    rewriter.replaceOp(op, outputTensors);
+    return success();
+  }
+};
+
+struct LegalizeSortPass
+    : public impl::HloLegalizeSortPassBase<LegalizeSortPass> {
+  // Perform the lowering to MLIR control flow.
+  void runOnOperation() override {
+    func::FuncOp f = getOperation();
+    MLIRContext* ctx = f.getContext();
+
+    RewritePatternSet patterns(ctx);
+    patterns.add<SortOpPattern>(ctx);
+
+    mlir::ConversionTarget target(*ctx);
+    target.markUnknownOpDynamicallyLegal([](Operation*) { return true; });
+    target.addIllegalOp<mhlo::SortOp>();
+
+    if (failed(applyPartialConversion(f, target, std::move(patterns)))) {
+      signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace mhlo
+}  // namespace mlir
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
+mlir::mhlo::createLegalizeSortPass() {
+  return std::make_unique<LegalizeSortPass>();
+}
diff --git a/mhlo/transforms/legalize_to_standard/legalize_to_standard.cc b/mhlo/transforms/legalize_to_standard/legalize_to_standard.cc
new file mode 100644
index 00000000..be752397
--- /dev/null
+++ b/mhlo/transforms/legalize_to_standard/legalize_to_standard.cc
@@ -0,0 +1,243 @@
+/* Copyright 2019 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// This file implements logic for lowering MHLO dialect to Standard dialect.
+
+#include <memory>
+#include <optional>
+#include <utility>
+
+#include "mhlo/IR/hlo_ops.h"
+#include "mhlo/transforms/passes.h"
+#include "mhlo/transforms/rewriters.h"
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Math/IR/Math.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Support/LLVM.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+namespace mlir {
+namespace {
+#include "legalize_to_standard/generated_legalize_to_standard.inc"
+}  // end anonymous namespace
+namespace mhlo {
+
+#define GEN_PASS_DEF_LEGALIZETOSTANDARDPASS
+#include "mhlo/transforms/mhlo_passes.h.inc"
+
+namespace {
+
+class CompareIConvert : public OpRewritePattern<mhlo::CompareOp> {
+ public:
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(mhlo::CompareOp op,
+                                PatternRewriter &rewriter) const override {
+    auto lhs = op.getLhs();
+    auto rhs = op.getRhs();
+    auto lhsType = mlir::cast<TensorType>(lhs.getType());
+    auto rhsType = mlir::cast<TensorType>(rhs.getType());
+
+    // Broadcasting not supported by this rewrite.
+    if (lhsType.getShape() != rhsType.getShape()) return failure();
+
+    if (!lhsType.getElementType().isSignlessInteger() ||
+        !rhsType.getElementType().isSignlessInteger())
+      return failure();
+
+    std::optional<arith::CmpIPredicate> comparePredicate = std::nullopt;
+    switch (op.getComparisonDirection()) {
+      case ComparisonDirection::EQ:
+        comparePredicate = arith::CmpIPredicate::eq;
+        break;
+      case ComparisonDirection::NE:
+        comparePredicate = arith::CmpIPredicate::ne;
+        break;
+      case ComparisonDirection::LT:
+        comparePredicate = arith::CmpIPredicate::slt;
+        break;
+      case ComparisonDirection::LE:
+        comparePredicate = arith::CmpIPredicate::sle;
+        break;
+      case ComparisonDirection::GT:
+        comparePredicate = arith::CmpIPredicate::sgt;
+        break;
+      case ComparisonDirection::GE:
+        comparePredicate = arith::CmpIPredicate::sge;
+        break;
+    }
+
+    if (!comparePredicate.has_value()) return failure();
+
+    rewriter.replaceOpWithNewOp<arith::CmpIOp>(op, comparePredicate.value(),
+                                               lhs, rhs);
+    return success();
+  }
+};
+
+class CompareFConvert : public OpRewritePattern<mhlo::CompareOp> {
+ public:
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(mhlo::CompareOp op,
+                                PatternRewriter &rewriter) const override {
+    auto lhs = op.getLhs();
+    auto rhs = op.getRhs();
+    auto lhsType = mlir::cast<TensorType>(lhs.getType());
+    auto rhsType = mlir::cast<TensorType>(rhs.getType());
+
+    // Broadcasting not supported by this rewrite.
+    if (lhsType.getShape() != rhsType.getShape()) return failure();
+
+    if (!mlir::isa<FloatType>(lhsType.getElementType()) ||
+        !mlir::isa<FloatType>(rhsType.getElementType()))
+      return failure();
+
+    std::optional<arith::CmpFPredicate> comparePredicate = std::nullopt;
+    switch (op.getComparisonDirection()) {
+      case ComparisonDirection::EQ:
+        comparePredicate = arith::CmpFPredicate::OEQ;
+        break;
+      case ComparisonDirection::NE:
+        comparePredicate = arith::CmpFPredicate::UNE;
+        break;
+      case ComparisonDirection::LT:
+        comparePredicate = arith::CmpFPredicate::OLT;
+        break;
+      case ComparisonDirection::LE:
+        comparePredicate = arith::CmpFPredicate::OLE;
+        break;
+      case ComparisonDirection::GT:
+        comparePredicate = arith::CmpFPredicate::OGT;
+        break;
+      case ComparisonDirection::GE:
+        comparePredicate = arith::CmpFPredicate::OGE;
+        break;
+    }
+
+    if (!comparePredicate.has_value()) return failure();
+
+    rewriter.replaceOpWithNewOp<arith::CmpFOp>(op, comparePredicate.value(),
+                                               lhs, rhs);
+    return success();
+  }
+};
+
+// Replace IotaOp with an integer constant. A ConvertOp is added to
+// convert the integer constant to iota result type. For complex types, the real
+// part is replaced with the generated constant and the imaginary part is
+// replaced with zero tensor.
+class ConvertIotaOp : public OpRewritePattern<mhlo::IotaOp> {
+ public:
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(mhlo::IotaOp op,
+                                PatternRewriter &rewriter) const override {
+    auto outputType = mlir::cast<ShapedType>(op.getType());
+    auto outputSize = outputType.getNumElements();
+    auto dimension = op.getIotaDimension();
+    auto maxDimSize = outputType.getDimSize(dimension);
+
+    auto elementType = outputType.getElementType();
+    int bitwidth;
+
+    auto complexTy = mlir::dyn_cast<ComplexType>(elementType);
+    Type intOrFloatTy = elementType;
+    if (complexTy) intOrFloatTy = complexTy.getElementType();
+
+    bitwidth = intOrFloatTy.getIntOrFloatBitWidth();
+    llvm::SmallVector<APInt, 10> values;
+    values.reserve(outputSize);
+
+    int64_t increaseStride = outputSize;
+    for (uint64_t i = 0; i <= dimension; i++) {
+      increaseStride /= outputType.getDimSize(i);
+    }
+
+    int64_t currentValue = 0;
+    for (int i = 0; i < outputSize; i++) {
+      int64_t value = (currentValue / increaseStride) % maxDimSize;
+      values.push_back(APInt(bitwidth, value));
+      ++currentValue;
+    }
+
+    auto intShapeType = RankedTensorType::get(
+        outputType.getShape(),
+        IntegerType::get(rewriter.getContext(), bitwidth));
+    auto loc = op.getLoc();
+    auto integerConst = rewriter.create<mlir::arith::ConstantOp>(
+        loc, DenseIntElementsAttr::get(intShapeType, values));
+
+    auto intOrFloatShapeTy =
+        RankedTensorType::get(outputType.getShape(), intOrFloatTy);
+
+    auto iotaConst =
+        rewriter.create<ConvertOp>(loc, intOrFloatShapeTy, integerConst);
+
+    // For int/float types we are done, replace op and return.
+    if (!complexTy) {
+      rewriter.replaceOp(op, iotaConst.getResult());
+      return success();
+    }
+
+    // For complex types, generate a constant tensor of zeroes for the imaginary
+    // part and use iota_const for real part.
+    auto zeroes = rewriter.create<mlir::arith::ConstantOp>(
+        loc, DenseIntElementsAttr::get(intShapeType, APInt(bitwidth, 0)));
+    auto imagZeroes =
+        rewriter.create<ConvertOp>(loc, intOrFloatShapeTy, zeroes);
+    rewriter.replaceOpWithNewOp<mhlo::ComplexOp>(op, iotaConst, imagZeroes);
+    return success();
+  }
+};
+
+}  // end anonymous namespace
+
+namespace {
+struct LegalizeToStandardPass
+    : public impl::LegalizeToStandardPassBase<LegalizeToStandardPass> {
+  void getDependentDialects(DialectRegistry &registry) const override {
+    registry
+        .insert<arith::ArithDialect, math::MathDialect, func::FuncDialect>();
+  }
+
+  /// Perform the lowering to Standard dialect.
+  void runOnOperation() override;
+};
+}  // end anonymous namespace
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
+createLegalizeToStdPass() {
+  return std::make_unique<LegalizeToStandardPass>();
+}
+
+void populateMhloToStdPatterns(RewritePatternSet *patterns,
+                               mlir::MLIRContext *ctx) {
+  mlir::populateWithGenerated(*patterns);
+  patterns->add<CompareFConvert, CompareIConvert, ConvertIotaOp>(ctx);
+}
+
+/// Perform the lowering to standard dialect.
+void LegalizeToStandardPass::runOnOperation() {
+  RewritePatternSet patterns(&getContext());
+  mlir::mhlo::populateMhloToStdPatterns(&patterns, &getContext());
+  if (failed(applyPatternsAndFoldGreedily(getOperation(), std::move(patterns))))
+    return signalPassFailure();
+}
+
+}  // end namespace mhlo
+}  // end namespace mlir
diff --git a/mhlo/transforms/legalize_to_standard/legalize_to_standard_patterns.td b/mhlo/transforms/legalize_to_standard/legalize_to_standard_patterns.td
new file mode 100644
index 00000000..f4d24608
--- /dev/null
+++ b/mhlo/transforms/legalize_to_standard/legalize_to_standard_patterns.td
@@ -0,0 +1,92 @@
+/* Copyright 2019 The OpenXLA Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// This is the legalization pattern definition file for MHLO to StandardOps.
+
+include "mlir/IR/OpBase.td"
+include "mlir/Dialect/Arith/IR/ArithOps.td"
+include "mlir/Dialect/Math/IR/MathOps.td"
+include "mlir/Dialect/Func/IR/FuncOps.td"
+include "mhlo/IR/hlo_ops.td"
+
+//===----------------------------------------------------------------------===//
+// Nullary op patterns.
+//===----------------------------------------------------------------------===//
+
+def : Pat<(MHLO_ConstantOp ElementsAttr:$value),
+          (Arith_ConstantOp $value)>;
+
+//===----------------------------------------------------------------------===//
+// Binary op patterns.
+//===----------------------------------------------------------------------===//
+
+def IsSameSizePred : CPred<
+    "cast<ShapedType>($0.getType()).getShape() "
+    "== cast<ShapedType>($1.getType()).getShape()">;
+def IsSameSizeConstraint : Constraint<IsSameSizePred, "inputs are same size">;
+def createFastMathNone : NativeCodeCall<
+    "::mlir::arith::FastMathFlagsAttr::get("
+      "$_builder.getContext(), ::mlir::arith::FastMathFlags::none"
+    ")">;
+def createOverflowNone : NativeCodeCall<
+    "::mlir::arith::IntegerOverflowFlagsAttr::get("
+      "$_builder.getContext(), ::mlir::arith::IntegerOverflowFlags::none"
+    ")">;
+
+
+// Unary Lowering Patterns.
+def : Pat<(MHLO_CeilOp MHLO_FpTensor:$i), (Math_CeilOp $i, (createFastMathNone ))>;
+
+// Binary Lowering Patterns.
+def : Pat<(MHLO_AndOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_AndIOp $l, $r),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_OrOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_OrIOp $l, $r),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_AddOp MHLO_FpTensor:$l, MHLO_FpTensor:$r),
+          (Arith_AddFOp $l, $r, (createFastMathNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_SubtractOp MHLO_FpTensor:$l, MHLO_FpTensor:$r),
+          (Arith_SubFOp $l, $r, (createFastMathNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_MulOp MHLO_FpTensor:$l, MHLO_FpTensor:$r),
+          (Arith_MulFOp $l, $r, (createFastMathNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_DivOp MHLO_FpTensor:$l, MHLO_FpTensor:$r),
+          (Arith_DivFOp $l, $r, (createFastMathNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_RemOp MHLO_FpTensor:$l, MHLO_FpTensor:$r),
+          (Arith_RemFOp $l, $r, (createFastMathNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_AddOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_AddIOp $l, $r, (createOverflowNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_SubtractOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_SubIOp $l, $r, (createOverflowNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_MulOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_MulIOp $l, $r, (createOverflowNone )),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_DivOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_DivSIOp $l, $r),
+          [(IsSameSizeConstraint $l, $r)]>;
+def : Pat<(MHLO_RemOp MHLO_IntTensor:$l, MHLO_IntTensor:$r),
+          (Arith_RemSIOp $l, $r),
+          [(IsSameSizeConstraint $l, $r)]>;
+
+def : Pat<(MHLO_SelectOp $pred, $tv, $fv),
+          (SelectOp $pred, $tv, $fv),
+          [(IsSameSizeConstraint $pred, $tv), (IsSameSizeConstraint $tv, $fv)]>;
diff --git a/mhlo/transforms/mhlo_passes.td b/mhlo/transforms/mhlo_passes.td
index 853531c1..378f8944 100644
--- a/mhlo/transforms/mhlo_passes.td
+++ b/mhlo/transforms/mhlo_passes.td
@@ -15,6 +15,25 @@ limitations under the License.
 
 include "mlir/Pass/PassBase.td"
 
+def LegalizeControlFlowPass : Pass<"mhlo-legalize-control-flow", "func::FuncOp"> {
+  let summary = "Legalize from MHLO control flow to SCF control flow.";
+  let constructor = "createLegalizeControlFlowPass()";
+  let dependentDialects = ["scf::SCFDialect", "tensor::TensorDialect"];
+}
+
+def LegalizeToStandardPass : Pass<"mhlo-legalize-to-std", "func::FuncOp"> {
+  let summary = "Legalize from MHLO dialect to standard dialect.";
+  let constructor = "createLegalizeToStdPass()";
+}
+
+def HloLegalizeSortPass : Pass<"hlo-legalize-sort", "func::FuncOp"> {
+  let summary = "Legalize from MHLO sort to SCF control flow.";
+  let constructor = "createLegalizeSortPass()";
+  let dependentDialects = ["arith::ArithDialect",
+                           "bufferization::BufferizationDialect",
+                           "scf::SCFDialect", "tensor::TensorDialect"];
+}
+
 def ChloLegalizeToHighLevelMhloPass : Pass<"chlo-legalize-to-high-level-mhlo", "func::FuncOp"> {
   let summary = "Legalize CHLO's with XLA counterparts, like TopK and Erf.";
   let description = [{
diff --git a/mhlo/transforms/passes.h b/mhlo/transforms/passes.h
index 3d2aa3b3..3f03b2df 100644
--- a/mhlo/transforms/passes.h
+++ b/mhlo/transforms/passes.h
@@ -37,6 +37,10 @@ namespace mhlo {
 #define GEN_PASS_DECL
 #include "mhlo/transforms/mhlo_passes.h.inc"
 
+std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeControlFlowPass();
+std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeSortPass();
+std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeToStdPass();
+
 /// Lowers from HLO dialect to Arithmetic dialect.
 std::unique_ptr<OperationPass<ModuleOp>> createLegalizeToArithmeticPass();
 
-- 
2.34.1

