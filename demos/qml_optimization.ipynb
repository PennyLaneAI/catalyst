{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfGlh7oY6lpx"
      },
      "source": [
        "# How to optimize a quantum machine learning model using Catalyst and Optax\n",
        "\n",
        "Once you have set up your quantum machine learning model (which typically includes deciding on your circuit architecture/ansatz, determining how you embed or integrate your data, and creating your cost function to minimize a quantity of interest), the next step is **optimization**. That is, setting up a classical optimization loop to find a minimal value of your cost function.\n",
        "\n",
        "In this example, we'll show you how to use [JAX](https://jax.readthedocs.io), an autodifferentiable machine learning framework, and [Optax](https://optax.readthedocs.io/), a suite of JAX-compatible gradient-based optimizers, to optimize a PennyLane quantum machine learning model.\n",
        "\n",
        "## Set up your model, data, and cost\n",
        "\n",
        "Here, we will create a simple QML model for our optimization. In particular:\n",
        "\n",
        "- We will embed our data through a series of rotation gates.\n",
        "- We will then have an ansatz of trainable rotation gates with parameters `weights`; it is these values we will train to minimize our cost function.\n",
        "- We will train the QML model on `data`, a `(5, 4)` array, and optimize the model to match target predictions given by `target`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iKtDMXjENR7X"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "from jax import numpy as jnp\n",
        "import optax\n",
        "import catalyst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tiK-itpFNVwE"
      },
      "outputs": [],
      "source": [
        "n_wires = 5\n",
        "data = jnp.sin(jnp.mgrid[-2:2:0.2].reshape(n_wires, -1)) ** 3\n",
        "targets = jnp.array([-0.2, 0.4, 0.35, 0.2])\n",
        "\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_wires)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w4QPSXdysqJx"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\n",
        "def circuit(data, weights):\n",
        "    \"\"\"Quantum circuit ansatz\"\"\"\n",
        "\n",
        "    @qml.for_loop(0, n_wires, 1)\n",
        "    def data_embedding(i):\n",
        "        qml.RY(data[i], wires=i)\n",
        "\n",
        "    data_embedding()\n",
        "\n",
        "    @qml.for_loop(0, n_wires, 1)\n",
        "    def ansatz(i):\n",
        "        qml.RX(weights[i, 0], wires=i)\n",
        "        qml.RY(weights[i, 1], wires=i)\n",
        "        qml.RX(weights[i, 2], wires=i)\n",
        "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
        "\n",
        "    ansatz()\n",
        "\n",
        "    # we use a sum of local Z's as an observable since a\n",
        "    # local Z would only be affected by params on that qubit.\n",
        "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UqaMlDcowNv"
      },
      "source": [
        "The `catalyst.vmap` function allows us to specify that the first argument to circuit (`data`) contains a batch dimension. In this example, the batch dimension is the second axis (axis 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uTeDby31ouoZ"
      },
      "outputs": [],
      "source": [
        "circuit = qml.qjit(catalyst.vmap(circuit, in_axes=(1, None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1bpU4Au6xUE"
      },
      "source": [
        "We will define a simple cost function that computes the overlap between model output and target data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0c7bwv3IsUxP"
      },
      "outputs": [],
      "source": [
        "def my_model(data, weights, bias):\n",
        "    return circuit(data, weights) + bias\n",
        "\n",
        "@qml.qjit\n",
        "def loss_fn(params, data, targets):\n",
        "    predictions = my_model(data, params[\"weights\"], params[\"bias\"])\n",
        "    loss = jnp.sum((targets - predictions) ** 2 / len(data))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUBCtuId62o9"
      },
      "source": [
        "Note that the model above is just an example for demonstration -- there are important considerations that must be taken into account when performing QML research, including methods for data embedding, circuit architecture, and cost function, in order to build models that may have use. This is still an active area of research; see our [demonstrations](https://pennylane.ai/qml/demonstrations) for details.\n",
        "\n",
        "## Initialize your parameters\n",
        "\n",
        "Now, we can generate our trainable parameters `weights` and `bias` that will be used to train our QML model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sOWgLW5esIsO"
      },
      "outputs": [],
      "source": [
        "weights = jnp.ones([n_wires, 3])\n",
        "bias = jnp.array(0.)\n",
        "params = {\"weights\": weights, \"bias\": bias}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mREqzeb7LI7"
      },
      "source": [
        "Plugging the trainable parameters, data, and target labels into our cost function, we can see the current loss as well as the parameter gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23t_bcyOtNBV",
        "outputId": "af3144ad-92e7-4b23-c0d4-ab213e84b9e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(0.29232612)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_fn(params, data, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCeL2_c4tPSi",
        "outputId": "36690a24-c429-41a9-a96c-7d6c682001ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bias': array(-0.75432067), 'weights': array([[-1.95077271e-01,  5.28546590e-02, -4.89252073e-01],\n",
            "       [-1.99687794e-02, -5.32871564e-02,  9.22904864e-02],\n",
            "       [-2.71755507e-03, -9.64672786e-05, -4.79570827e-03],\n",
            "       [-6.35443870e-02,  3.61110014e-02, -2.05196876e-01],\n",
            "       [-9.02635405e-02,  1.63759364e-01, -5.64262612e-01]])}\n"
          ]
        }
      ],
      "source": [
        "print(qml.qjit(catalyst.grad(loss_fn, method=\"fd\"))(params, data, targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzRbmfJv7PC_"
      },
      "source": [
        "## Create the optimizer\n",
        "\n",
        "We can now use Optax to create an Adam optimizer, and train our circuit.\n",
        "\n",
        "We first define our `update_step` function, which needs to do a couple of things:\n",
        "\n",
        "- Compute the loss function (so we can track training) and the gradients (so we can apply an optimization step). We can do this in one execution via the `jax.value_and_grad` function.\n",
        "\n",
        "- Apply the update step of our optimizer via `opt.update`\n",
        "\n",
        "- Update the parameters via `optax.apply_updates`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D8bnRCgG82Of"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer we want to work with\n",
        "opt = optax.adam(learning_rate=0.3)\n",
        "\n",
        "@qml.qjit\n",
        "def update_step(i, args):\n",
        "    params, opt_state, data, targets = args\n",
        "\n",
        "    grads = catalyst.grad(loss_fn, method=\"fd\")(params, data, targets)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return (params, opt_state, data, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIuvvT6r89fF",
        "outputId": "c1322367-416b-413c-b56f-922d456816e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0 Loss: 0.2730353743615765\n",
            "Step: 5 Loss: 0.032559240964478334\n",
            "Step: 10 Loss: 0.02928202254673762\n",
            "Step: 15 Loss: 0.03337864936356614\n",
            "Step: 20 Loss: 0.031236313936757257\n",
            "Step: 25 Loss: 0.02719147809590714\n",
            "Step: 30 Loss: 0.022688382285209845\n",
            "Step: 35 Loss: 0.018162726468822145\n",
            "Step: 40 Loss: 0.014789693761705976\n",
            "Step: 45 Loss: 0.01120695897423158\n",
            "Step: 50 Loss: 0.009409797491512443\n",
            "Step: 55 Loss: 0.017898242851615405\n",
            "Step: 60 Loss: 0.012861310065141539\n",
            "Step: 65 Loss: 0.009916026349390799\n",
            "Step: 70 Loss: 0.008611660326780315\n",
            "Step: 75 Loss: 0.006585500811217603\n",
            "Step: 80 Loss: 0.006778125695692339\n",
            "Step: 85 Loss: 0.00604372270143045\n",
            "Step: 90 Loss: 0.006139651693718838\n",
            "Step: 95 Loss: 0.00498953052913176\n"
          ]
        }
      ],
      "source": [
        "loss_history = []\n",
        "\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "for i in range(100):\n",
        "    params, opt_state, _, _ = update_step(i, (params, opt_state, data, targets))\n",
        "    loss_val = loss_fn(params, data, targets)\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Step: {i} Loss: {loss_val}\")\n",
        "\n",
        "    loss_history.append(loss_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99kM0UDs83QM"
      },
      "source": [
        "## JIT-compiling the optimization\n",
        "\n",
        "In the above example, we just-in-time (JIT) compiled our cost function `loss_fn`. However, we can also JIT compile the entire optimization loop; this means that the for-loop around optimization is not happening in Python, but is compiled and executed natively. This avoids (potentially costly) data transfer between Python and our JIT compiled cost function with each update step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nZjOeD-79zKT"
      },
      "outputs": [],
      "source": [
        "params = {\"weights\": weights, \"bias\": bias}\n",
        "\n",
        "@qml.qjit\n",
        "def optimization(params, data, targets):\n",
        "    opt_state = opt.init(params)\n",
        "    args = (params, opt_state, data, targets)\n",
        "    (params, opt_state, _, _) = catalyst.for_loop(0, 100, 1)(update_step)(args)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsFXWti89HQ_"
      },
      "source": [
        "Note that we use `catalyst.for_loop` rather than a standard Python for loop, to allow the control flow to be JIT compatible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gVRxMBgg2nGF"
      },
      "outputs": [],
      "source": [
        "final_params = optimization(params, data, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSJlW1rx2tyR",
        "outputId": "3db6a16f-7cfb-45c6-a826-c5c513987413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bias': array(-0.75292885), 'weights': array([[ 1.63086995,  1.55018972,  0.6721261 ],\n",
            "       [ 0.7266062 ,  0.36422543, -0.756247  ],\n",
            "       [ 2.78387487,  0.62721014,  3.44996393],\n",
            "       [-1.10119515, -0.12679488,  0.89283774],\n",
            "       [ 1.27236329,  1.10631134,  2.22051434]])}\n"
          ]
        }
      ],
      "source": [
        "print(final_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr47uMdmooVg"
      },
      "source": [
        "## Timing the optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2I1r0Bm9mzp"
      },
      "source": [
        "We can time the two approaches (JIT compiling just the cost function, vs JIT compiling the entire optimization loop) to explore the differences in performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I9Yaeyer9rVb"
      },
      "outputs": [],
      "source": [
        "opt = optax.adam(learning_rate=0.3)\n",
        "\n",
        "def optimization_noqjit(params):\n",
        "    opt_state = opt.init(params)\n",
        "\n",
        "    for i in range(100):\n",
        "        params, opt_state, _, _ = update_step(i, (params, opt_state, data, targets))\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RdNHtoW904i",
        "outputId": "579cb8aa-296e-4d34-87f8-3837a3d4e731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.16 s ± 453 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "optimization_noqjit(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muq-IfE95KNa",
        "outputId": "797d4e08-1803-4ac7-a1df-a405773ffbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "631 ms ± 148 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit optimization(params, data, targets)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
